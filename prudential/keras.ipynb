{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu0 is not available  (error: Unable to get the number of gpus available: no CUDA-capable device is detected)\n",
      "WARNING:theano.sandbox.cuda:CUDA is installed, but device gpu0 is not available  (error: Unable to get the number of gpus available: no CUDA-capable device is detected)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Creating dataset...\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "File ../input/train.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3fc867586d91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating dataset...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0museDummies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfillNANStrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0museNormalization\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputShape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adadelta'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'glorot_normal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-3fc867586d91>\u001b[0m in \u001b[0;36mmake_dataset\u001b[1;34m(useDummies, fillNANStrategy, useNormalization)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0museDummies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfillNANStrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0museNormalization\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../input/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[0;32m    463\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3143)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5765)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File ../input/train.csv does not exist"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "class NN:\n",
    "    #I made a small wrapper for the Keras model to make it more scikit-learn like\n",
    "    #I think they have something like this built in already, oh well\n",
    "    #See http://keras.io/ for parameter options\n",
    "    def __init__(self, inputShape, layers, dropout = [], activation = 'relu', init = 'uniform', loss = 'rmse', optimizer = 'adadelta', nb_epochs = 50, batch_size = 32, verbose = 1):\n",
    "\n",
    "        model = Sequential()\n",
    "        for i in range(len(layers)):\n",
    "            if i == 0:\n",
    "                print (\"Input shape: \" + str(inputShape))\n",
    "                print (\"Adding Layer \" + str(i) + \": \" + str(layers[i]))\n",
    "                model.add(Dense(layers[i], input_dim = inputShape, init = init))\n",
    "            else:\n",
    "                print (\"Adding Layer \" + str(i) + \": \" + str(layers[i]))\n",
    "                model.add(Dense(layers[i], init = init))\n",
    "            print (\"Adding \" + activation + \" layer\")\n",
    "            model.add(Activation(activation))\n",
    "            model.add(BatchNormalization())\n",
    "            if len(dropout) > i:\n",
    "                print (\"Adding \" + str(dropout[i]) + \" dropout\")\n",
    "                model.add(Dropout(dropout[i]))\n",
    "        model.add(Dense(1, init = init)) #End in a single output node for regression style output\n",
    "        model.compile(loss=loss, optimizer=optimizer)\n",
    "        \n",
    "        self.model = model\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y): \n",
    "        self.model.fit(X.values, y.values, nb_epoch=self.nb_epochs, batch_size=self.batch_size, verbose = self.verbose)\n",
    "        \n",
    "    def predict(self, X, batch_size = 128, verbose = 1):\n",
    "        return self.model.predict(X.values, batch_size = batch_size, verbose = verbose)\n",
    "\n",
    "class pdStandardScaler:\n",
    "    #Applies the sklearn StandardScaler to pandas dataframes\n",
    "    def __init__(self):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        self.StandardScaler = StandardScaler()\n",
    "    def fit(self, df):\n",
    "        self.StandardScaler.fit(df)\n",
    "    def transform(self, df):\n",
    "        df = pd.DataFrame(self.StandardScaler.transform(df), columns=df.columns)\n",
    "        return df\n",
    "    def fit_transform(self, df):\n",
    "        df = pd.DataFrame(self.StandardScaler.fit_transform(df), columns=df.columns)\n",
    "        return df\n",
    "        \n",
    "def getDummiesInplace(columnList, train, test = None):\n",
    "    #Takes in a list of column names and one or two pandas dataframes\n",
    "    #One-hot encodes all indicated columns inplace\n",
    "    columns = []\n",
    "    \n",
    "    if test is not None:\n",
    "        df = pd.concat([train,test], axis= 0)\n",
    "    else:\n",
    "        df = train\n",
    "        \n",
    "    for columnName in df.columns:\n",
    "        index = df.columns.get_loc(columnName)\n",
    "        if columnName in columnList:\n",
    "            dummies = pd.get_dummies(df.ix[:,index], prefix = columnName, prefix_sep = \".\")\n",
    "            columns.append(dummies)\n",
    "        else:\n",
    "            columns.append(df.ix[:,index])\n",
    "    df = pd.concat(columns, axis = 1)\n",
    "    \n",
    "    if test is not None:\n",
    "        train = df[:train.shape[0]]\n",
    "        test = df[train.shape[0]:]\n",
    "        return train, test\n",
    "    else:\n",
    "        train = df\n",
    "        return train\n",
    "        \n",
    "def pdFillNAN(df, strategy = \"mean\"):\n",
    "    #Fills empty values with either the mean value of each feature, or an indicated number\n",
    "    if strategy == \"mean\":\n",
    "        return df.fillna(df.mean())\n",
    "    elif type(strategy) == int:\n",
    "        return df.fillna(strategy)\n",
    "        \n",
    "def make_dataset(useDummies = True, fillNANStrategy = \"mean\", useNormalization = True):\n",
    "    data_dir = \"\"\n",
    "    train = pd.read_csv(data_dir + 'train.csv')\n",
    "    test = pd.read_csv(data_dir + 'test.csv')\n",
    "    \n",
    "    labels = train[\"Response\"]\n",
    "    train.drop(labels = \"Id\", axis = 1, inplace = True)\n",
    "    train.drop(labels = \"Response\", axis = 1, inplace = True)\n",
    "    test.drop(labels = \"Id\", axis = 1, inplace = True)\n",
    "    \n",
    "    categoricalVariables = [\"Product_Info_1\", \"Product_Info_2\", \"Product_Info_3\", \"Product_Info_5\", \"Product_Info_6\", \"Product_Info_7\", \"Employment_Info_2\", \"Employment_Info_3\", \"Employment_Info_5\", \"InsuredInfo_1\", \"InsuredInfo_2\", \"InsuredInfo_3\", \"InsuredInfo_4\", \"InsuredInfo_5\", \"InsuredInfo_6\", \"InsuredInfo_7\", \"Insurance_History_1\", \"Insurance_History_2\", \"Insurance_History_3\", \"Insurance_History_4\", \"Insurance_History_7\", \"Insurance_History_8\", \"Insurance_History_9\", \"Family_Hist_1\", \"Medical_History_2\", \"Medical_History_3\", \"Medical_History_4\", \"Medical_History_5\", \"Medical_History_6\", \"Medical_History_7\", \"Medical_History_8\", \"Medical_History_9\", \"Medical_History_10\", \"Medical_History_11\", \"Medical_History_12\", \"Medical_History_13\", \"Medical_History_14\", \"Medical_History_16\", \"Medical_History_17\", \"Medical_History_18\", \"Medical_History_19\", \"Medical_History_20\", \"Medical_History_21\", \"Medical_History_22\", \"Medical_History_23\", \"Medical_History_25\", \"Medical_History_26\", \"Medical_History_27\", \"Medical_History_28\", \"Medical_History_29\", \"Medical_History_30\", \"Medical_History_31\", \"Medical_History_33\", \"Medical_History_34\", \"Medical_History_35\", \"Medical_History_36\", \"Medical_History_37\", \"Medical_History_38\", \"Medical_History_39\", \"Medical_History_40\", \"Medical_History_41\"]\n",
    "\n",
    "    if useDummies == True:\n",
    "        print (\"Generating dummies...\")\n",
    "        train, test = getDummiesInplace(categoricalVariables, train, test)\n",
    "    \n",
    "    if fillNANStrategy is not None:\n",
    "        print (\"Filling in missing values...\")\n",
    "        train = pdFillNAN(train, fillNANStrategy)\n",
    "        test = pdFillNAN(test, fillNANStrategy)\n",
    "\n",
    "    if useNormalization == True:\n",
    "        print (\"Scaling...\")\n",
    "        scaler = pdStandardScaler()\n",
    "        train = scaler.fit_transform(train)\n",
    "        test = scaler.transform(test)\n",
    "    \n",
    "    return train, test, labels\n",
    "\n",
    "print (\"Creating dataset...\") \n",
    "train, test, labels = make_dataset(useDummies = True, fillNANStrategy = \"mean\", useNormalization = True)\n",
    "    \n",
    "clf = NN(inputShape = train.shape[1], layers = [128, 64], dropout = [0.5, 0.5], loss='mae', optimizer = 'adadelta', init = 'glorot_normal', nb_epochs = 5)\n",
    "\n",
    "print (\"Training model...\")\n",
    "clf.fit(train, labels)\n",
    "\n",
    "print (\"Making predictions...\")\n",
    "pred = clf.predict(test)\n",
    "predClipped = np.clip(np.round(pred), 1, 8).astype(int) #Make the submissions within the accepted range\n",
    "\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission[\"Response\"] = predClipped\n",
    "submission.to_csv('NNSubmission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
