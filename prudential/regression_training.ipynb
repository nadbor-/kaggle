{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier, SGDRegressor, Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from xgboost.sklearn import XGBClassifier, XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from persistent_cache import memo, PersistentDict as Perd\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "\n",
    "def eval_wrapper(yhat, y):  \n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n",
    "    return quadratic_weighted_kappa(yhat, y)\n",
    "\n",
    "#TODO: check which ones are really worth encoding and which can be even dropped (some may contain nonoverlapping values between\n",
    "#test and training sest)\n",
    "categorical = {'Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', 'Product_Info_6', \n",
    "               'Product_Info_7', 'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', \n",
    "               'InsuredInfo_1', 'InsuredInfo_2', 'InsuredInfo_3', 'InsuredInfo_4', 'InsuredInfo_5', \n",
    "               'InsuredInfo_6', 'InsuredInfo_7', 'Insurance_History_1', 'Insurance_History_2', \n",
    "               'Insurance_History_3', 'Insurance_History_4', 'Insurance_History_7', 'Insurance_History_8', \n",
    "               'Insurance_History_9', 'Family_Hist_1', 'Medical_History_2', 'Medical_History_3', \n",
    "               'Medical_History_4', 'Medical_History_5', 'Medical_History_6', 'Medical_History_7', \n",
    "               'Medical_History_8', 'Medical_History_9', 'Medical_History_10', 'Medical_History_11', \n",
    "               'Medical_History_12', 'Medical_History_13', 'Medical_History_14', 'Medical_History_16', \n",
    "               'Medical_History_17', 'Medical_History_18', 'Medical_History_19', 'Medical_History_20', \n",
    "               'Medical_History_21', 'Medical_History_22', 'Medical_History_23', 'Medical_History_25', \n",
    "               'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29', \n",
    "               'Medical_History_30', 'Medical_History_31', 'Medical_History_33', 'Medical_History_34', \n",
    "               'Medical_History_35', 'Medical_History_36', 'Medical_History_37', \n",
    "#                'Medical_History_38', \n",
    "               'Medical_History_39', 'Medical_History_40', 'Medical_History_41','Medical_History_1', \n",
    "               'Medical_History_15', 'Medical_History_24', 'Medical_History_32'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# create logger\n",
    "logger = logging.getLogger(\"logging_tryout2\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "# ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter\n",
    "# formatter = logging.Formatter(\"%(asctime)s;%(levelname)s;%(message)s\")\n",
    "formatter = logging.Formatter(\"%(asctime)s;%(levelname)s;%(message)s\",\n",
    "                              \"%Y-%m-%d %H:%M:%S\")\n",
    "# add formatter to ch\n",
    "ch.setFormatter(formatter)\n",
    "# logger.addHandler(ch)\n",
    "def info(msg):\n",
    "    logger.info(msg.replace(\"\\n\", \"  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "total = pd.concat([train, test])\n",
    "median = total.median()\n",
    "train.fillna(median, inplace=True)\n",
    "test = test.fillna(median, inplace=True)\n",
    "encoder = LabelEncoder()\n",
    "for f in categorical:\n",
    "    encoder.fit(total[f])\n",
    "    train[f] = encoder.transform(train[f])\n",
    "    test[f] = encoder.transform(test[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_cols = test.columns[1:]\n",
    "categorical_inds = [i for i, col in enumerate(feature_cols) if col in categorical]\n",
    "oh_encoder = OneHotEncoder(categorical_features=categorical_inds)\n",
    "\n",
    "X = np.array(train[test.columns[1:]])\n",
    "y = np.array(train.Response)\n",
    "X_actual_test = np.array(test[feature_cols])\n",
    "\n",
    "oh_encoder.fit(X)\n",
    "X = oh_encoder.transform(X).todense()\n",
    "X_actual_test = oh_encoder.transform(X_actual_test).todense()\n",
    "\n",
    "train_test_folds = list(StratifiedKFold(y, n_folds=6, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@memo(Perd(\"memo/train_predictions\"))\n",
    "def train_predictions(model):\n",
    "    ind2pred = {}\n",
    "    for i, (train, test) in enumerate(train_test_folds):\n",
    "        info((\"fitting fold   \"+str(i+1)+ str(model)[:100]))\n",
    "        model.fit(X[train], y[train])\n",
    "        info((\"fold fitted    \"+str(i+1)+  str(model)[:100]))\n",
    "        preds = model.predict(X[test])\n",
    "        for i, p in zip(test, preds):\n",
    "            ind2pred[i] = p\n",
    "    \n",
    "    return np.array([ind2pred[i] for i in range(len(y))])\n",
    "\n",
    "@memo(Perd(\"memo/test_predictions\"))\n",
    "def test_predictions(model):\n",
    "    model.fit(X, y)\n",
    "    return model.predict(X_actual_test)\n",
    "\n",
    "\n",
    "@memo(Perd(\"memo/stacker_train_predictions\"))\n",
    "def stacker_train_predictions(stacker, base_clfs):\n",
    "    info(\"start stacker --------------------------\")\n",
    "    n = len(y)\n",
    "    stacked_X = np.hstack([X] + [train_predictions(clf).reshape(n, 1) for clf in base_clfs])\n",
    "    info(\"base regressors done\")\n",
    "    ind2pred = {}\n",
    "    for i, (train, test) in enumerate(train_test_folds):\n",
    "        info(\"fitting stacker fold %s   %s\" % (i, str(stacker)))\n",
    "\n",
    "        stacker.fit(stacked_X[train], y[train])\n",
    "        info(\"stacker fitted fold %s    %s \" % (i, str(stacker)))\n",
    "        preds = stacker.predict(stacked_X[test])\n",
    "        for i, p in zip(test, preds):\n",
    "            ind2pred[i] = p\n",
    "    info(\"stacker done =========================\")\n",
    "    return np.array([ind2pred[i] for i in range(len(y))])\n",
    "\n",
    "@memo(Perd(\"memo/lazy_stacker_train_predictions\"))\n",
    "def lazy_stacker_train_predictions(stacker, base_clfs):\n",
    "    info(\"start stacker --------------------------\")\n",
    "    n = len(y)\n",
    "    stacked_X = np.hstack([train_predictions(clf).reshape(n, 1) for clf in base_clfs])\n",
    "    info(\"base regressors done\")\n",
    "    ind2pred = {}\n",
    "    for i, (train, test) in enumerate(train_test_folds):\n",
    "        info(\"fitting stacker fold %s   %s\" % (i, str(stacker)))\n",
    "\n",
    "        stacker.fit(stacked_X[train], y[train])\n",
    "        info(\"stacker fitted fold %s    %s \" % (i, str(stacker)))\n",
    "        preds = stacker.predict(stacked_X[test])\n",
    "        for i, p in zip(test, preds):\n",
    "            ind2pred[i] = p\n",
    "    info(\"stacker done =========================\")\n",
    "    return np.array([ind2pred[i] for i in range(len(y))])\n",
    "\n",
    "\n",
    "@memo(Perd(\"memo/stacker_test_predictions\"))\n",
    "def stacker_test_predictions(stacker, base_clfs):\n",
    "    n = len(y)\n",
    "    stacked_X = np.hstack([X] + [train_predictions(clf).reshape(n, 1) for clf in base_clfs])\n",
    "    stacker.fit(stacked_X, y)\n",
    "    n = X_actual_test.shape[0]\n",
    "    stacked_test_X = np.hstack([X_actual_test] + [test_predictions(clf).reshape(n, 1) for clf in base_clfs])\n",
    "    return stacker.predict(stacked_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark(model):\n",
    "    pred = train_predictions(model)\n",
    "    return eval_wrapper(pred, y)\n",
    "\n",
    "def make_predictions(model):\n",
    "    model.fit(X, y)\n",
    "    return model.predict(X_actual_test)\n",
    "\n",
    "def benchmark_stacker(model, base_clfs):\n",
    "    pred = stacker_train_predictions(model, base_clfs)\n",
    "    return eval_wrapper(pred, y)\n",
    "\n",
    "\n",
    "def benchmark_lazy_stacker(model, base_clfs):\n",
    "    pred = lazy_stacker_train_predictions(model, base_clfs)\n",
    "    return eval_wrapper(pred, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgbr = lambda: XGBRegressor(objective=\"reg:linear\", min_child_weight=80, subsample=0.85, colsample_bytree=0.30, silent=1, max_depth=9)\n",
    "xgbc = lambda: XGBClassifier(objective=\"reg:linear\", min_child_weight=80, subsample=0.85, colsample_bytree=0.30, silent=1, max_depth=9)\n",
    "rfr = lambda: RandomForestRegressor(n_estimators=400)\n",
    "etr = lambda: ExtraTreesRegressor(n_estimators=400)\n",
    "etc = lambda: ExtraTreesClassifier(n_estimators=400)\n",
    "sgdr = lambda: SGDRegressor()\n",
    "perceptron = lambda: Perceptron()\n",
    "\n",
    "dream_team = lambda: sorted([xgbr(), rfr(),  etr(), sgdr(), LinearRegression(), Perceptron(),\n",
    "              SVR(kernel=\"linear\"), SVR(kernel=\"poly\"), SVR(kernel=\"sigmoid\"),\n",
    "              SVR(kernel=\"rbf\"), LogisticRegression()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 1, 1, ..., 4, 8, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions(DummyClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data using pandas\n",
      "Eliminate missing values\n"
     ]
    }
   ],
   "source": [
    "print(\"Load the data using pandas\")\n",
    "xtrain = pd.read_csv(\"train.csv\")\n",
    "xtest = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# combine train and test\n",
    "xall_data = xtrain.append(test)\n",
    "\n",
    "# factorize categorical variables    \n",
    "xall_data['Product_Info_2'] = pd.factorize(xall_data['Product_Info_2'])[0]\n",
    "\n",
    "print('Eliminate missing values')    \n",
    "# Use -1 for any others\n",
    "xall_data.fillna(-1, inplace=True)\n",
    "\n",
    "# fix the dtype on the label column\n",
    "xall_data['Response'] = xall_data['Response'].astype(int)\n",
    "\n",
    "# Provide split column\n",
    "# all_data['Split'] = np.random.randint(5, size=all_data.shape[0])\n",
    "\n",
    "# split train and test\n",
    "xtrain = xall_data[xall_data['Response']>0].copy()\n",
    "xtest = xall_data[xall_data['Response']<1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xftrain = np.array(xtrain.drop([\"Id\", \"Response\"], axis=1))\n",
    "xftest = np.array(xtrain.drop([\"Id\", \"Response\"], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xy = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 4, 8, ..., 8, 8, 7])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(xtrain.Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, compute_importances=None,\n",
       "           criterion='mse', max_depth=None, max_features='auto',\n",
       "           max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, n_estimators=400, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1000\n",
    "model = RandomForestRegressor(n_estimators=400)\n",
    "model.fit(xftrain[:n, :], y[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, normalize=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1000\n",
    "model = RandomForestRegressor(n_estimators=400)\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(xftrain[:n, :], y[:n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-74af9a54840c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msgdr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxftrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxftrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    873\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m                          \u001b[0mintercept_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 875\u001b[1;33m                          sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    842\u001b[0m         return self._partial_fit(X, y, alpha, C, loss, learning_rate,\n\u001b[0;32m    843\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m                                  coef_init, intercept_init)\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m     def fit(self, X, y, coef_init=None, intercept_init=None,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_partial_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, n_iter, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         self._fit_regressor(X, y, alpha, C, loss, learning_rate,\n\u001b[1;32m--> 797\u001b[1;33m                             sample_weight, n_iter)\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_fit_regressor\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, sample_weight, n_iter)\u001b[0m\n\u001b[0;32m    937\u001b[0m                                           \u001b[0mlearning_rate_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meta0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m                                           intercept_decay)\n\u001b[0m\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintercept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msgd_fast.pyx\u001b[0m in \u001b[0;36msklearn.linear_model.sgd_fast.plain_sgd (sklearn/linear_model/sgd_fast.c:5842)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help."
     ]
    }
   ],
   "source": [
    "n=100000\n",
    "for model in [sgdr()]:\n",
    "    model.fit(xftrain[:n, :], y[:n])\n",
    "    preds = model.predict(xftrain[:n, :])\n",
    "    print ((preds - y[:n])**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.48495861181\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.112510735757\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(xftrain, y)\n",
    "preds = model.predict(xftrain)\n",
    "print ((preds - y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "    def decider(all_scores):\n",
    "        \"\"\"\n",
    "        Each bit from left to right represents the list below,\n",
    "        1. director different from company name\n",
    "        2. director same as company name x\n",
    "        3. url (primary|secondary)\n",
    "        4. infoboxcompany (proof of being a company)\n",
    "        5. company_name > X similarity\n",
    "        6. company_name_match\n",
    "\n",
    "        The way the regex is put together represents what different combinations can pass,\n",
    "\n",
    "        A. Director different from company name\n",
    "        B. Director same as company name and (url (primary or secondary) or proof of it being a company)\n",
    "        C. url(primary or secondary) and company name > X similarity with titles/redirect titles\n",
    "        D. company_name_match and proof of it being a company\n",
    "        \"\"\"\n",
    "\n",
    "        acceptance_criteria = [\n",
    "            re.compile(ur'1.....'),\n",
    "            re.compile(ur'.1(10|01|11)..'),\n",
    "            re.compile(ur'..1.1.'),\n",
    "            # re.compile(ur'...1.1')\n",
    "        ]\n",
    "\n",
    "        ranking_titles = [\n",
    "            'Distinct director name(different from company name)',\n",
    "            'Director name equal to company name and (company url or proof of being a company)',\n",
    "            'Url match and company name similar to wikipedia titles/redirect titles or company parent from infobox',\n",
    "            'Company name matches wikipedia titles/redirect titles and the page can be a company page, and incorporation date is in infobox'\n",
    "        ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
