{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MEMO_PATH = \"memo_fe/\"\n",
    "\n",
    "#=========================================LOGGING\n",
    "import logging\n",
    "# create logger\n",
    "logging.basicConfig(filename='best_ensemble.log',level=logging.DEBUG, format=\"%(asctime)s; %(levelname)s;  %(message)s\")\n",
    "logger = logging.getLogger(\"trainlo\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "def info(msg):\n",
    "    logger.info(msg.replace(\"\\n\", \"  \"))\n",
    "#=========================================LOGGING\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.optimize import fmin_powell\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier, \\\n",
    "    SGDRegressor, Perceptron, PassiveAggressiveRegressor, BayesianRidge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, SVR\n",
    "from xgboost.sklearn import XGBClassifier, XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from persistent_cache import memo, PersistentDict as Perd\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "from feature_engineering import train_test_sets\n",
    "\n",
    "def eval_wrapper(yhat, y):  \n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n",
    "    return quadratic_weighted_kappa(yhat, y)\n",
    "\n",
    "\n",
    "num_classes = 8\n",
    "# print(\"Load the data using pandas\")\n",
    "# train = pd.read_csv(\"train.csv\")\n",
    "# test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# # combine train and test\n",
    "# all_data = train.append(test)\n",
    "\n",
    "# # factorize categorical variables    \n",
    "# all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\n",
    "\n",
    "# # FEATURE ENGINEERING\n",
    "# all_data['bmi_ins_age'] = all_data.BMI * all_data.Ins_Age\n",
    "# all_data['nan_count'] = all_data.isnull().sum(axis=1)\n",
    "# #all_data['emp_inf_4_sq'] = all_data.Employment_Info_4 ** 2\n",
    "# #all_data['fam_hist_4_sq'] = all_data.Family_Hist_4 ** 2\n",
    "# #all_data['fam_hist_2_sq'] = all_data.Family_Hist_2 ** 2\n",
    "\n",
    "# mk = [col for col in train.columns if col.startswith(\"Medical_K\")]\n",
    "# all_data['sum_keywords'] = sum(train[col] for col in mk)\n",
    "\n",
    "# all_data.drop('Medical_History_24')\n",
    "# all_data.drop('Medical_History_10')\n",
    "\n",
    "\n",
    "\n",
    "# print('Eliminate missing values')    \n",
    "# # Use -1 for any others\n",
    "# all_data.fillna(-1, inplace=True)\n",
    "\n",
    "# # fix the dtype on the label column\n",
    "# all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "# # Provide split column\n",
    "# # all_data['Split'] = np.random.randint(5, size=all_data.shape[0])\n",
    "\n",
    "# # split train and test\n",
    "# train = all_data[all_data['Response']>0].copy()\n",
    "# test = all_data[all_data['Response']<1].copy()\n",
    "\n",
    "\n",
    "# X = np.array(train.drop([\"Id\", \"Response\"], axis=1))\n",
    "# X_actual_test = np.array(test.drop([\"Id\", \"Response\"], axis=1))\n",
    "# y = np.array(train.Response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = np.array(pd.read_csv(\"train.csv\").Response)\n",
    "train_test_folds = list(StratifiedKFold(y, n_folds=4, random_state=0))\n",
    "#================================================================================================\n",
    "@memo(Perd(MEMO_PATH + \"_train_predictions\"))\n",
    "def train_predictions(model, fe):\n",
    "    X, _ = train_test_sets(fe)\n",
    "    ind2pred = {}\n",
    "    for i, (train, test) in enumerate(train_test_folds):\n",
    "        info((\"fitting fold   \"+str(i+1)+ str(model)[:100]))\n",
    "        model.fit(X[train], y[train])\n",
    "        info((\"fold fitted    \"+str(i+1)+  str(model)[:100]))\n",
    "        preds = model.predict(X[test])\n",
    "        for i, p in zip(test, preds):\n",
    "            ind2pred[i] = p\n",
    "    \n",
    "    return np.array([ind2pred[i] for i in range(len(y))])\n",
    "\n",
    "@memo(Perd(MEMO_PATH + \"_test_predictions\"))\n",
    "def test_predictions(model, fe):\n",
    "    X, X_actual_test = train_test_sets(fe)\n",
    "    info(\"fitting (on full train set) %s\" % model)\n",
    "    model.fit(X, y)\n",
    "    info(\"done fitting for %s\" % model)\n",
    "    return model.predict(X_actual_test)\n",
    "\n",
    "\n",
    "@memo(Perd(MEMO_PATH + \"_stacker_train_predictions\"))\n",
    "def stacker_train_predictions(stacker, base_clfs, fe):\n",
    "    X, _ = train_test_sets(fe)\n",
    "    info(\"start stacker --------------------------\")\n",
    "    n = len(y)\n",
    "    stacked_X = np.hstack([X] + [train_predictions(clf, fe).reshape(n, 1) for clf in base_clfs])\n",
    "    info(\"base regressors done\")\n",
    "    ind2pred = {}\n",
    "    for i, (train, test) in enumerate(train_test_folds):\n",
    "        info(\"fitting stacker fold %s   %s\" % (i, str(stacker)))\n",
    "\n",
    "        stacker.fit(stacked_X[train], y[train])\n",
    "        info(\"stacker fitted fold %s    %s \" % (i, str(stacker)))\n",
    "        preds = stacker.predict(stacked_X[test])\n",
    "        for i, p in zip(test, preds):\n",
    "            ind2pred[i] = p\n",
    "    info(\"stacker done =========================\")\n",
    "    return np.array([ind2pred[i] for i in range(len(y))])\n",
    "\n",
    "@memo(Perd(MEMO_PATH + \"_lazy_stacker_train_predictions\"))\n",
    "def lazy_stacker_train_predictions(stacker, base_clfs, fe):\n",
    "    info(\"start stacker --------------------------\")\n",
    "    n = len(y)\n",
    "    stacked_X = np.hstack([train_predictions(clf, fe).reshape(n, 1) for clf in base_clfs])\n",
    "    info(\"base regressors done\")\n",
    "    ind2pred = {}\n",
    "    for i, (train, test) in enumerate(train_test_folds):\n",
    "        info(\"fitting stacker fold %s   %s\" % (i, str(stacker)))\n",
    "\n",
    "        stacker.fit(stacked_X[train], y[train])\n",
    "        info(\"stacker fitted fold %s    %s \" % (i, str(stacker)))\n",
    "        preds = stacker.predict(stacked_X[test])\n",
    "        for i, p in zip(test, preds):\n",
    "            ind2pred[i] = p\n",
    "    info(\"stacker done =========================\")\n",
    "    return np.array([ind2pred[i] for i in range(len(y))])\n",
    "\n",
    "\n",
    "@memo(Perd(MEMO_PATH + \"_stacker_test_predictions\"))\n",
    "def stacker_test_predictions(stacker, base_clfs, fe):\n",
    "    n = len(y)\n",
    "    stacked_X = np.hstack([X] + [train_predictions(clf, fe).reshape(n, 1) for clf in base_clfs])\n",
    "    stacker.fit(stacked_X, y)\n",
    "    nn = X_actual_test.shape[0]\n",
    "    stacked_test_X = np.hstack([X_actual_test] + [test_predictions(clf, fe).reshape(nn, 1) for clf in base_clfs])\n",
    "    return stacker.predict(stacked_test_X)\n",
    "#============================================================================\n",
    "def benchmark(model, fe):\n",
    "    pred = train_predictions(model, fe)\n",
    "    return eval_wrapper(pred, y)\n",
    "\n",
    "def make_predictions(model, fe):\n",
    "    X, X_actual_test = train_test_sets(fe)\n",
    "    model.fit(X, y)\n",
    "    return model.predict(X_actual_test)\n",
    "\n",
    "def benchmark_stacker(model, base_clfs, fe):\n",
    "    pred = stacker_train_predictions(model, base_clfs, fe)\n",
    "    result = eval_wrapper(pred, y)\n",
    "    info(\"stacker %.4f   %s, %s, feats = %s\" % (result, model, base_clfs, fe))\n",
    "    return result\n",
    "\n",
    "def benchmark_lazy_stacker(model, base_clfs, fe):\n",
    "    pred = lazy_stacker_train_predictions(model, base_clfs, fe)\n",
    "    result = eval_wrapper(pred, y)\n",
    "    info(\"lazy stacker %.4f   %s, %s  feats = %s\" % (result, model, base_clfs, fe))\n",
    "    \n",
    "    return result\n",
    "#==============================================================================\n",
    "# OPTIMISING OFFSETS\n",
    "# -----------------------------------------------------------------------------\n",
    "def apply_offset(data, bin_offset, sv, scorer=eval_wrapper):\n",
    "    # data has the format of pred=0, offset_pred=1, labels=2 in the first dim\n",
    "    data[1, data[0].astype(int)==sv] = data[0, data[0].astype(int)==sv] + bin_offset\n",
    "    score = scorer(data[1], data[2])\n",
    "    return score\n",
    "\n",
    "\n",
    "def optimize_offsets(predictions, y):\n",
    "    # train offsets\n",
    "    info(\"optimising offsets %s\" % len(y))\n",
    "    offsets = np.ones(num_classes) * -0.5\n",
    "    offset_train_preds = np.vstack((predictions, predictions, y))\n",
    "    \n",
    "    for j in range(num_classes):\n",
    "        train_offset = lambda x: -apply_offset(offset_train_preds, x, j)\n",
    "        offsets[j] = fmin_powell(train_offset, offsets[j], disp=False) \n",
    "    info(\"done optimising offsets %s\" % len(y))\n",
    "    return offsets\n",
    "\n",
    "def actually_apply_offsets(predictions, offsets):\n",
    "    data = np.vstack((predictions, predictions, -np.ones(len(predictions))))\n",
    "    for j in range(num_classes):\n",
    "        data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j] \n",
    "\n",
    "    final_test_preds = np.round(np.clip(data[1], 1, 8)).astype(int)\n",
    "    return final_test_preds\n",
    "\n",
    "def optimized_train_predictions(raw_train_predictions):\n",
    "    n = len(y)\n",
    "    ind2pred = {}\n",
    "    for i, (train, test) in enumerate(train_test_folds):\n",
    "        train_preds = raw_train_predictions[train]\n",
    "        offsets = optimize_offsets(train_preds, y[train])\n",
    "        test_preds = actually_apply_offsets(raw_train_predictions[test], offsets)\n",
    "        for i, p in zip(test, test_preds):\n",
    "            ind2pred[i] = p\n",
    "    return np.array([ind2pred[i] for i in range(len(y))])\n",
    "\n",
    "def benchmark_model_optimized(model, fe):\n",
    "    preds = optimized_train_predictions(train_predictions(model, fe))\n",
    "    result = eval_wrapper(preds, y)\n",
    "    info(\"optimized %.4f   %s  %s\" % (result, model, fe))\n",
    "    return result\n",
    "\n",
    "def benchmark_optimized_stacker(stacker, base_clfs, fe):\n",
    "    preds = stacker_train_predictions(stacker, base_clfs, fe)\n",
    "    opreds = optimized_train_predictions(preds)\n",
    "    result = eval_wrapper(opreds, y)\n",
    "    info(\"optimized stacker %.4f   %s, %s,  feats=%s\" % (result, stacker, base_clfs, fe))\n",
    "    return result\n",
    "\n",
    "def benchmark_optimized_lazy_stacker(stacker, base_clfs, fe):\n",
    "    preds = lazy_stacker_train_predictions(stacker, base_clfs, fe)\n",
    "    opreds = optimized_train_predictions(preds)\n",
    "    result = eval_wrapper(opreds, y)\n",
    "    info(\"optimized stacker %.4f   %s, %s   feats=%s\" % (result, stacker, base_clfs, fe))\n",
    "    return result\n",
    "\n",
    "def optimized_test_predictions(stacker, base_clfs, fe):\n",
    "    train_preds = stacker_train_predictions(stacker, base_clfs, fe)\n",
    "    offsets = optimize_offsets(train_preds, y)\n",
    "    test_preds = stacker_test_predictions(stacker, base_clfs, fe)\n",
    "    final_test_preds = actually_apply_offsets(test_preds, offsets)\n",
    "    #print \"print len(train_preds), len(test_preds), len(final_test_preds)\"\n",
    "    #print len(train_preds), len(test_preds), len(final_test_preds)\n",
    "    info(\"made optimized predictions for stacker %s with features %s and base regressors %s\" % (stacker, fe, base_clfs))\n",
    "    return final_test_preds\n",
    "\n",
    "def make_sub_optimized(stacker, base_clfs, fe, filename):\n",
    "    preds = optimized_test_predictions(stacker, base_clfs, fe)\n",
    "    #print \"len(preds)\"\n",
    "    #print len(preds)\n",
    "    df = pd.DataFrame()\n",
    "    df['Id'] = test.Id\n",
    "    df['Response'] = preds\n",
    "    info(\"made submission to file %s. stacker %s, features %s\" % (filename, stacker, fe))\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "def make_sub(stacker, base_clfs, fe, filename):\n",
    "    preds = stacker_test_predictions(stacker, base_clfs, fe)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['Id'] = test.Id\n",
    "    df['Response'] = preds\n",
    "    info(\"making stacker %s, nonoptimized submission to file %s \" % (stacker, filename))\n",
    "    df.to_csv(filename, index=False)\n",
    "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "xgbr = lambda: XGBRegressor(objective=\"reg:linear\", min_child_weight=80, subsample=0.85, colsample_bytree=0.30, silent=1, max_depth=9)\n",
    "xgbc = lambda: XGBClassifier(objective=\"reg:linear\", min_child_weight=80, subsample=0.85, colsample_bytree=0.30, silent=1, max_depth=9)\n",
    "rfr = lambda: RandomForestRegressor(n_estimators=400)\n",
    "etr = lambda: ExtraTreesRegressor(n_estimators=400)\n",
    "etc = lambda: ExtraTreesClassifier(n_estimators=400)\n",
    "sgdr = lambda: SGDRegressor()\n",
    "xgbr_poly = lambda: Pipeline([(\"poly\", PolynomialFeatures(degree=2)), (\"xgbr\", xgbr())])\n",
    "linreg_poly = lambda: Pipeline([(\"poly\", PolynomialFeatures(degree=2)), (\"linreg\", LinearRegression())])\n",
    "linreg = lambda: LinearRegression()\n",
    "bayes_ridge = lambda: BayesianRidge()\n",
    "lasso = lambda: Lasso()\n",
    "svrsig = lambda: SVR(kernel=\"sigmoid\")\n",
    "svrrbf = lambda: SVR(kernel=\"rbf\")\n",
    "perc = lambda: Perceptron()\n",
    "\n",
    "dream_team = lambda: sorted([xgbr(), rfr(),  etr(), LinearRegression(), \n",
    "                             #xgbr_poly(), linreg_poly(),\n",
    "                             bayes_ridge(),\n",
    "                             lasso(),\n",
    "                             svrsig(),\n",
    "                             #svrrbf(),\n",
    "                             perc()\n",
    "                            ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:trainlo:=-=-=-=-=-=-=-=-=-=-=-=-=- mic test, disregard  =-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "INFO:trainlo:fitting fold   1BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "INFO:trainlo:fold fitted    1BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "INFO:trainlo:fitting fold   2BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "INFO:trainlo:fold fitted    2BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "INFO:trainlo:fitting fold   3BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "INFO:trainlo:fold fitted    3BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "INFO:trainlo:fitting fold   4BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "INFO:trainlo:fold fitted    4BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5241169466704168"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info(\"=-=-=-=-=-=-=-=-=-=-=-=-=- mic test, disregard  =-=-=-=-=-=-=-=-=-=-=-=-=-\")\n",
    "benchmark(BayesianRidge(), \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "screen -d -m -S traininho python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a screen on:\r\n",
      "\t2424..ip-172-31-3-184\t(01/17/2016 03:03:41 PM)\t(Detached)\n",
      "1 Socket in /var/run/screen/S-ubuntu.\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "screen -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data using pandas\n",
      "Eliminate missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 29, in <module>\n",
      "    bos(LinearRegression(), dream_team())\n",
      "  File \"/home/ubuntu/nadbor/kaggle/prudential/train_utils.py\", line 232, in benchmark_optimized_stacker\n",
      "    preds = stacker_train_predictions(stacker, base_clfs)\n",
      "  File \"<decorator-gen-3>\", line 2, in stacker_train_predictions\n",
      "  File \"/home/ubuntu/nadbor/kaggle/prudential/persistent_cache.py\", line 103, in wrapper\n",
      "    cache[key] = result = f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/nadbor/kaggle/prudential/train_utils.py\", line 120, in stacker_train_predictions\n",
      "    stacked_X = np.hstack([X] + [train_predictions(clf).reshape(n, 1) for clf in base_clfs])\n",
      "  File \"<decorator-gen-1>\", line 2, in train_predictions\n",
      "  File \"/home/ubuntu/nadbor/kaggle/prudential/persistent_cache.py\", line 103, in wrapper\n",
      "    cache[key] = result = f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/nadbor/kaggle/prudential/train_utils.py\", line 100, in train_predictions\n",
      "    model.fit(X[train], y[train])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/sklearn/pipeline.py\", line 129, in fit\n",
      "    Xt, fit_params = self._pre_transform(X, y, **fit_params)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/sklearn/pipeline.py\", line 119, in _pre_transform\n",
      "    Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/sklearn/base.py\", line 429, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py\", line 496, in transform\n",
      "    return (X[:, None, :] ** self.powers_).prod(-1)\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-24 22:37:44,913; INFO;  start train.py $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "2016-01-24 22:37:44,915; INFO;  start stacker --------------------------\n",
      "2016-01-24 22:37:44,916; INFO;  fitting fold   1BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "2016-01-24 22:37:45,803; INFO;  fold fitted    1BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "2016-01-24 22:37:45,825; INFO;  fitting fold   2BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "2016-01-24 22:37:46,730; INFO;  fold fitted    2BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "2016-01-24 22:37:46,747; INFO;  fitting fold   3BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "2016-01-24 22:37:47,668; INFO;  fold fitted    3BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "2016-01-24 22:37:47,685; INFO;  fitting fold   4BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "2016-01-24 22:37:48,577; INFO;  fold fitted    4BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=T\n",
      "2016-01-24 22:37:48,725; INFO;  fitting fold   1ExtraTreesRegressor(bootstrap=False, compute_importances=None,            criterion='mse', max_depth=\n",
      "2016-01-24 22:50:52,782; INFO;  fold fitted    1ExtraTreesRegressor(bootstrap=False, compute_importances=None,            criterion='mse', max_depth=\n",
      "2016-01-24 22:50:54,828; INFO;  fitting fold   2ExtraTreesRegressor(bootstrap=False, compute_importances=None,            criterion='mse', max_depth=\n",
      "2016-01-24 23:03:39,108; INFO;  fold fitted    2ExtraTreesRegressor(bootstrap=False, compute_importances=None,            criterion='mse', max_depth=\n",
      "2016-01-24 23:03:41,099; INFO;  fitting fold   3ExtraTreesRegressor(bootstrap=False, compute_importances=None,            criterion='mse', max_depth=\n",
      "2016-01-24 23:16:05,612; INFO;  fold fitted    3ExtraTreesRegressor(bootstrap=False, compute_importances=None,            criterion='mse', max_depth=\n",
      "2016-01-24 23:16:07,594; INFO;  fitting fold   4ExtraTreesRegressor(bootstrap=False, compute_importances=None,            criterion='mse', max_depth=\n",
      "2016-01-24 23:28:38,602; INFO;  fold fitted    4ExtraTreesRegressor(bootstrap=False, compute_importances=None,            criterion='mse', max_depth=\n",
      "2016-01-24 23:28:40,649; INFO;  fitting fold   1Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,     normalize=False, positive=False,\n",
      "2016-01-24 23:28:40,925; INFO;  fold fitted    1Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,     normalize=False, positive=False,\n",
      "2016-01-24 23:28:40,943; INFO;  fitting fold   2Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,     normalize=False, positive=False,\n",
      "2016-01-24 23:28:41,233; INFO;  fold fitted    2Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,     normalize=False, positive=False,\n",
      "2016-01-24 23:28:41,250; INFO;  fitting fold   3Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,     normalize=False, positive=False,\n",
      "2016-01-24 23:28:41,540; INFO;  fold fitted    3Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,     normalize=False, positive=False,\n",
      "2016-01-24 23:28:41,557; INFO;  fitting fold   4Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,     normalize=False, positive=False,\n",
      "2016-01-24 23:28:41,847; INFO;  fold fitted    4Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,     normalize=False, positive=False,\n",
      "2016-01-24 23:28:41,996; INFO;  fitting fold   1LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
      "2016-01-24 23:28:42,268; INFO;  fold fitted    1LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
      "2016-01-24 23:28:42,281; INFO;  fitting fold   2LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
      "2016-01-24 23:28:42,583; INFO;  fold fitted    2LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
      "2016-01-24 23:28:42,596; INFO;  fitting fold   3LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
      "2016-01-24 23:28:42,896; INFO;  fold fitted    3LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
      "2016-01-24 23:28:42,909; INFO;  fitting fold   4LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
      "2016-01-24 23:28:43,207; INFO;  fold fitted    4LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
      "2016-01-24 23:28:43,348; INFO;  fitting fold   1Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,        n_iter=5, n_jobs=1, \n",
      "2016-01-24 23:28:43,757; INFO;  fold fitted    1Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,        n_iter=5, n_jobs=1, \n",
      "2016-01-24 23:28:43,778; INFO;  fitting fold   2Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,        n_iter=5, n_jobs=1, \n",
      "2016-01-24 23:28:44,214; INFO;  fold fitted    2Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,        n_iter=5, n_jobs=1, \n",
      "2016-01-24 23:28:44,232; INFO;  fitting fold   3Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,        n_iter=5, n_jobs=1, \n",
      "2016-01-24 23:28:44,670; INFO;  fold fitted    3Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,        n_iter=5, n_jobs=1, \n",
      "2016-01-24 23:28:44,688; INFO;  fitting fold   4Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,        n_iter=5, n_jobs=1, \n",
      "2016-01-24 23:28:45,125; INFO;  fold fitted    4Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,        n_iter=5, n_jobs=1, \n",
      "2016-01-24 23:28:45,310; INFO;  fitting fold   1Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), (\n",
      "2016-01-25 16:09:05,483; INFO;  start train.py $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "2016-01-25 16:09:05,485; INFO;  start stacker --------------------------\n",
      "2016-01-25 16:09:05,620; INFO;  fitting fold   1Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), (\n",
      "2016-01-25 16:18:51,360; INFO;  start train.py $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "2016-01-25 16:18:51,361; INFO;  start stacker --------------------------\n",
      "2016-01-25 16:18:51,498; INFO;  fitting fold   1Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), (\n",
      "2016-01-27 21:34:02,886; INFO;  start train.py $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "2016-01-27 21:34:02,888; INFO;  start stacker --------------------------\n",
      "2016-01-27 21:34:03,024; INFO;  fitting fold   1Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), (\n",
      "2016-01-27 21:38:05,389; INFO;  start train.py $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "2016-01-27 21:38:05,391; INFO;  start stacker --------------------------\n",
      "2016-01-27 21:38:05,526; INFO;  fitting fold   1Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), (\n",
      "2016-01-27 21:38:22,070; INFO;  start train.py $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "2016-01-27 21:38:22,072; INFO;  start stacker --------------------------\n",
      "2016-01-27 21:38:22,208; INFO;  fitting fold   1Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), (\n",
      "2016-01-27 21:44:04,223; INFO;  start train.py $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "2016-01-27 21:44:04,225; INFO;  start stacker --------------------------\n",
      "2016-01-27 21:44:04,361; INFO;  fitting fold   1RandomForestRegressor(bootstrap=True, compute_importances=None,             criterion='mse', max_dept\n",
      "2016-01-27 21:56:33,274; INFO;  fold fitted    1RandomForestRegressor(bootstrap=True, compute_importances=None,             criterion='mse', max_dept\n",
      "2016-01-27 21:56:34,999; INFO;  fitting fold   2RandomForestRegressor(bootstrap=True, compute_importances=None,             criterion='mse', max_dept\n",
      "2016-01-27 22:08:58,513; INFO;  fold fitted    2RandomForestRegressor(bootstrap=True, compute_importances=None,             criterion='mse', max_dept\n",
      "2016-01-27 22:09:00,183; INFO;  fitting fold   3RandomForestRegressor(bootstrap=True, compute_importances=None,             criterion='mse', max_dept\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat best_ensemble.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             total       used       free     shared    buffers     cached\n",
      "Mem:          7984       2395       5588          0        151        459\n",
      "-/+ buffers/cache:       1784       6199\n",
      "Swap:            0          0          0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
       "        fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "        normalize=False, tol=0.001, verbose=False),\n",
       " ExtraTreesRegressor(bootstrap=False, compute_importances=None,\n",
       "           criterion='mse', max_depth=None, max_features='auto',\n",
       "           max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, n_estimators=400, n_jobs=1, oob_score=False,\n",
       "           random_state=None, verbose=0),\n",
       " Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "    normalize=False, positive=False, precompute='auto', tol=0.0001,\n",
       "    warm_start=False),\n",
       " LinearRegression(copy_X=True, fit_intercept=True, normalize=False),\n",
       " Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "       n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=False,\n",
       "       verbose=0, warm_start=False),\n",
       " RandomForestRegressor(bootstrap=True, compute_importances=None,\n",
       "            criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, n_estimators=400, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0),\n",
       " SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
       "   kernel='sigmoid', max_iter=-1, probability=False, random_state=None,\n",
       "   shrinking=True, tol=0.001, verbose=False),\n",
       " SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
       "   kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "   shrinking=True, tol=0.001, verbose=False),\n",
       " XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.3,\n",
       "        gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=9,\n",
       "        min_child_weight=80, missing=None, n_estimators=100, nthread=-1,\n",
       "        objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "        scale_pos_weight=1, seed=0, silent=1, subsample=0.85)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dream_team = lambda: sorted([xgbr(), rfr(),  etr(), LinearRegression(), \n",
    "                             #xgbr_poly(), linreg_poly(),\n",
    "                             bayes_ridge(),\n",
    "                             lasso(),\n",
    "                             svrsig(),\n",
    "                             svrrbf(),\n",
    "                             perc()\n",
    "                            ])\n",
    "dream_team()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2016-01-28 00:37:09,485; INFO;  optimized stacker 0.663023467567   \n",
    "        LinearRegression(copy_X=True, fit_intercept=True, normalize=False), \n",
    "        [\n",
    "            BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,         fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,         normalize=False, tol=0.001, verbose=False), \n",
    "            ExtraTreesRegressor(bootstrap=False, compute_importances=None,            criterion='mse', max_depth=None, max_features='auto',            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,            min_samples_split=2, n_estimators=400, n_jobs=1, oob_score=False,            random_state=None, verbose=0), \n",
    "            Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,     normalize=False, positive=False, precompute='auto', tol=0.0001,     warm_start=False), \n",
    "            LinearRegression(copy_X=True, fit_intercept=True, normalize=False), \n",
    "            Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,        n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=False,        verbose=0, warm_start=False), \n",
    "            RandomForestRegressor(bootstrap=True, compute_importances=None,             criterion='mse', max_depth=None, max_features='auto',             max_leaf_nodes=None, min_density=None, min_samples_leaf=1,             min_samples_split=2, n_estimators=400, n_jobs=1,             oob_score=False, random_state=None, verbose=0), \n",
    "            SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,    kernel='sigmoid', max_iter=-1, probability=False, random_state=None,    shrinking=True, tol=0.001, verbose=False), \n",
    "            SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,    kernel='rbf', max_iter=-1, probability=False, random_state=None,    shrinking=True, tol=0.001, verbose=False), \n",
    "            XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.3,         gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=9,         min_child_weight=80, missing=None, n_estimators=100, nthread=-1,         objective='reg:linear', reg_alpha=0, reg_lambda=1,         scale_pos_weight=1, seed=0, silent=1, subsample=0.85)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2016-01-28 00:38:12,708; INFO;  optimized stacker 0.663071898749   \n",
    "        LinearRegression(copy_X=True, fit_intercept=True, normalize=False), \n",
    "        [\n",
    "            ExtraTreesRegressor(bootstrap=False, compute_importances=None,            criterion='mse', max_depth=None, max_features='auto',            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,            min_samples_split=2, n_estimators=400, n_jobs=1, oob_score=False,            random_state=None, verbose=0), \n",
    "            Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,     normalize=False, positive=False, precompute='auto', tol=0.0001,     warm_start=False), \n",
    "            LinearRegression(copy_X=True, fit_intercept=True, normalize=False), \n",
    "            Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,        n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=False,        verbose=0, warm_start=False), \n",
    "            RandomForestRegressor(bootstrap=True, compute_importances=None,             criterion='mse', max_depth=None, max_features='auto',             max_leaf_nodes=None, min_density=None, min_samples_leaf=1,             min_samples_split=2, n_estimators=400, n_jobs=1,             oob_score=False, random_state=None, verbose=0), \n",
    "            SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,    kernel='sigmoid', max_iter=-1, probability=False, random_state=None,    shrinking=True, tol=0.001, verbose=False), \n",
    "            SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,    kernel='rbf', max_iter=-1, probability=False, random_state=None,    shrinking=True, tol=0.001, verbose=False), \n",
    "            XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.3,         gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=9,         min_child_weight=80, missing=None, n_estimators=100, nthread=-1,         objective='reg:linear', reg_alpha=0, reg_lambda=1,         scale_pos_weight=1, seed=0, silent=1, subsample=0.85)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-bayes ridge  0.663071898749\n",
    "-extra trees  0.662692889992\n",
    "-lasso        0.663020255307\n",
    "-linear       0.662786333124\n",
    "-perceptron   0.663053416635\n",
    "-randomforest 0.662830924589\n",
    "-svr sigmoid  0.663023467567\n",
    "-svr rbf      0.66318712225\n",
    "-xgb          0.656280329093\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5093228   0.44824519  0.65131449  0.83269357  0.8128499 ]\n"
     ]
    }
   ],
   "source": [
    "def fake_data():\n",
    "    theta = np.random.random(5)\n",
    "    X = np.random.random((1000, 5))\n",
    "    y = X.dot(theta)\n",
    "    maksio = max(y)\n",
    "    minio = min(y)\n",
    "    y = (num_classes - 1) * (1/(maksio - minio)) * (y - minio) + 1\n",
    "    y = np.round(y)\n",
    "    test_X = -X\n",
    "    return X, y, test_X\n",
    "\n",
    "iks, igrek, testiks = fake_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mol = LinearRegression().fit(iks, igrek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3660634 ,  1.22204675,  1.80184071,  2.30295306,  2.22929648])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.68756706,  0.37507146,  0.19799844,  0.83495337,  0.23041633])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from persistent_cache import memo, PersistentDict as Perd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def read_all_data():\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    # combine train and test\n",
    "    all_data = train.append(test)\n",
    "\n",
    "    # factorize categorical variables    \n",
    "    all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\n",
    "    return all_data\n",
    "\n",
    "def basic_extractor():\n",
    "    all_data = read_all_data()\n",
    "    \n",
    "    print('Eliminate missing values')    \n",
    "    # Use -1 for any others\n",
    "    all_data.fillna(-1, inplace=True)\n",
    "\n",
    "    # fix the dtype on the label column\n",
    "    all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "    # split train and test\n",
    "    train = all_data[all_data['Response']>0].copy()\n",
    "    test = all_data[all_data['Response']<1].copy()\n",
    "\n",
    "\n",
    "    X = np.array(train.drop([\"Id\", \"Response\"], axis=1))\n",
    "    X_actual_test = np.array(test.drop([\"Id\", \"Response\"], axis=1))\n",
    "    y = np.array(train.Response)\n",
    "    return X, y, X_actual_test\n",
    "\n",
    "\n",
    "def fe1():\n",
    "    \"\"\"not one-hot encoded\n",
    "    \"\"\"\n",
    "    all_data = read_all_data()\n",
    "\n",
    "    # FEATURE ENGINEERING\n",
    "    all_data['bmi_ins_age'] = all_data.BMI * all_data.Ins_Age\n",
    "    all_data['nan_count'] = all_data.isnull().sum(axis=1)\n",
    "    #all_data['emp_inf_4_sq'] = all_data.Employment_Info_4 ** 2\n",
    "    #all_data['fam_hist_4_sq'] = all_data.Family_Hist_4 ** 2\n",
    "    #all_data['fam_hist_2_sq'] = all_data.Family_Hist_2 ** 2\n",
    "\n",
    "    mk = [col for col in train.columns if col.startswith(\"Medical_K\")]\n",
    "    all_data['sum_keywords'] = sum(train[col] for col in mk)\n",
    "\n",
    "    all_data.drop('Medical_History_24')\n",
    "    all_data.drop('Medical_History_10')\n",
    "\n",
    "    # Use -1 for any others\n",
    "    all_data.fillna(-1, inplace=True)\n",
    "\n",
    "    # fix the dtype on the label column\n",
    "    all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "    # Provide split column\n",
    "    # all_data['Split'] = np.random.randint(5, size=all_data.shape[0])\n",
    "\n",
    "    # split train and test\n",
    "    train = all_data[all_data['Response']>0].copy()\n",
    "    test = all_data[all_data['Response']<1].copy()\n",
    "\n",
    "\n",
    "    X = np.array(train.drop([\"Id\", \"Response\"], axis=1))\n",
    "    X_actual_test = np.array(test.drop([\"Id\", \"Response\"], axis=1))\n",
    "    y = np.array(train.Response)\n",
    "    return X, y, X_actual_test\n",
    "\n",
    "\n",
    "extractors = {\n",
    "    'basic': basic_extractor,\n",
    "    'fake_data': fake_data,\n",
    "    'feats1': fe1\n",
    "}\n",
    "\n",
    "@memo(Perd(MEMO_PATH + \"feature_extraction\"))\n",
    "def train_test_sets(extractor_name):\n",
    "    extractor = extractors[extractor_name]\n",
    "    return extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from persistent_cache import memo, PersistentDict as Perd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import Normalizer\n",
    "MEMO_PATH = \"memo_fe/\"\n",
    "\n",
    "def oh_med():\n",
    "    categorical = {'Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', 'Product_Info_6', \n",
    "                   'Product_Info_7', 'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', \n",
    "                   'InsuredInfo_1', 'InsuredInfo_2', 'InsuredInfo_3', 'InsuredInfo_4', 'InsuredInfo_5', \n",
    "                   'InsuredInfo_6', 'InsuredInfo_7', 'Insurance_History_1', 'Insurance_History_2', \n",
    "                   'Insurance_History_3', 'Insurance_History_4', 'Insurance_History_7', 'Insurance_History_8', \n",
    "                   'Insurance_History_9', 'Family_Hist_1', 'Medical_History_2', 'Medical_History_3', \n",
    "                   'Medical_History_4', 'Medical_History_5', 'Medical_History_6', 'Medical_History_7', \n",
    "                   'Medical_History_8', 'Medical_History_9', 'Medical_History_10', 'Medical_History_11', \n",
    "                   'Medical_History_12', 'Medical_History_13', 'Medical_History_14', 'Medical_History_16', \n",
    "                   'Medical_History_17', 'Medical_History_18', 'Medical_History_19', 'Medical_History_20', \n",
    "                   'Medical_History_21', 'Medical_History_22', 'Medical_History_23', 'Medical_History_25', \n",
    "                   'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29', \n",
    "                   'Medical_History_30', 'Medical_History_31', 'Medical_History_33', 'Medical_History_34', \n",
    "                   'Medical_History_35', 'Medical_History_36', 'Medical_History_37', \n",
    "    #                'Medical_History_38', \n",
    "                   'Medical_History_39', 'Medical_History_40', 'Medical_History_41','Medical_History_1', \n",
    "                   'Medical_History_15', 'Medical_History_24', 'Medical_History_32'}\n",
    "\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "    total = pd.concat([train, test])\n",
    "    median = total.median()\n",
    "    train.fillna(median, inplace=True)\n",
    "    test = test.fillna(median, inplace=True)\n",
    "    encoder = LabelEncoder()\n",
    "    for f in categorical:\n",
    "        encoder.fit(total[f])\n",
    "        train[f] = encoder.transform(train[f])\n",
    "        test[f] = encoder.transform(test[f])\n",
    "\n",
    "    feature_cols = test.columns[1:]\n",
    "    categorical_inds = [i for i, col in enumerate(feature_cols) if col in categorical]\n",
    "    oh_encoder = OneHotEncoder(categorical_features=categorical_inds)\n",
    "\n",
    "    X = np.array(train[test.columns[1:]])\n",
    "    y = np.array(train.Response)\n",
    "    X_actual_test = np.array(test[feature_cols])\n",
    "\n",
    "    oh_encoder.fit(X)\n",
    "    X = oh_encoder.transform(X).todense()\n",
    "    X_actual_test = oh_encoder.transform(X_actual_test).todense()\n",
    "    return X, X_actual_test\n",
    "\n",
    "def read_all_data():\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    # combine train and test\n",
    "    all_data = train.append(test)\n",
    "\n",
    "    # factorize categorical variables    \n",
    "    all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\n",
    "    return all_data\n",
    "\n",
    "def basic_extractor():\n",
    "    all_data = read_all_data()\n",
    "    \n",
    "    print('Eliminate missing values')    \n",
    "    # Use -1 for any others\n",
    "    all_data.fillna(-1, inplace=True)\n",
    "\n",
    "    # fix the dtype on the label column\n",
    "    all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "    # split train and test\n",
    "    train = all_data[all_data['Response']>0].copy()\n",
    "    test = all_data[all_data['Response']<1].copy()\n",
    "\n",
    "    X = np.array(train.drop([\"Id\", \"Response\"], axis=1))\n",
    "    X_actual_test = np.array(test.drop([\"Id\", \"Response\"], axis=1))\n",
    "    y = np.array(train.Response)\n",
    "    return X, X_actual_test\n",
    "\n",
    "\n",
    "def fe1():\n",
    "    \"\"\"not one-hot encoded\n",
    "    \"\"\"\n",
    "    all_data = read_all_data()\n",
    "\n",
    "    # FEATURE ENGINEERING\n",
    "    all_data['bmi_ins_age'] = all_data.BMI * all_data.Ins_Age\n",
    "    all_data['nan_count'] = all_data.isnull().sum(axis=1)\n",
    "    #all_data['emp_inf_4_sq'] = all_data.Employment_Info_4 ** 2\n",
    "    #all_data['fam_hist_4_sq'] = all_data.Family_Hist_4 ** 2\n",
    "    #all_data['fam_hist_2_sq'] = all_data.Family_Hist_2 ** 2\n",
    "\n",
    "    mk = [col for col in train.columns if col.startswith(\"Medical_K\")]\n",
    "    all_data['sum_keywords'] = sum(train[col] for col in mk)\n",
    "\n",
    "    all_data.drop('Medical_History_24')\n",
    "    all_data.drop('Medical_History_10')\n",
    "\n",
    "    # Use -1 for any others\n",
    "    all_data.fillna(-1, inplace=True)\n",
    "\n",
    "    # fix the dtype on the label column\n",
    "    all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "    # Provide split column\n",
    "    # all_data['Split'] = np.random.randint(5, size=all_data.shape[0])\n",
    "\n",
    "    # split train and test\n",
    "    train = all_data[all_data['Response']>0].copy()\n",
    "    test = all_data[all_data['Response']<1].copy()\n",
    "\n",
    "\n",
    "    X = np.array(train.drop([\"Id\", \"Response\"], axis=1))\n",
    "    X_actual_test = np.array(test.drop([\"Id\", \"Response\"], axis=1))\n",
    "    y = np.array(train.Response)\n",
    "    return X, X_actual_test\n",
    "\n",
    "\n",
    "def kmeans_feats(X, clusters=10):\n",
    "    X = Normalizer().transform(X)\n",
    "    kmeans = MiniBatchKMeans(n_clusters=clusters, random_state=0)\n",
    "    return kmeans.fit_transform(X)\n",
    "    \n",
    "def kmeans_10():\n",
    "    X, X_test = train_test_sets(\"ohmed\")\n",
    "    n, _ = X.shape\n",
    "    tot = np.vstack([X, X_test])\n",
    "    kmeans = kmeans_feats(tot)\n",
    "    tot = np.hstack([tot, kmeans])\n",
    "    return tot[:n, :], tot[n:, :]\n",
    "    \n",
    "def kmeans_20():\n",
    "    X, X_test = train_test_sets(\"ohmed\")\n",
    "    n, _ = X.shape\n",
    "    tot = np.vstack([X, X_test])\n",
    "    kmeans = kmeans_feats(tot, clusters=20)\n",
    "    tot = np.hstack([tot, kmeans])\n",
    "    return tot[:n, :], tot[n:, :]\n",
    "\n",
    "def combine(fextractors):\n",
    "    trains, tests = [], []\n",
    "    for train, test in [train_test_sets(f) for f in fextractors]:\n",
    "        trains.append(train)\n",
    "        tests.append(test)\n",
    "    return np.hstack(trains), np.hstack(tests)    \n",
    "    \n",
    "def oh_km10():\n",
    "    return combine([\"ohmed\", \"kmeans10\"])\n",
    "    \n",
    "def oh_km20():\n",
    "    return combine([\"ohmed\", \"kmeans20\"])\n",
    "\n",
    "def fe1_km10():\n",
    "    return combine([\"feats1\", \"kmeans10\"])\n",
    "\n",
    "def fe1_km20():\n",
    "    return combine([\"feats1\", \"kmeans20\"])\n",
    "\n",
    "\n",
    "extractors = {\n",
    "    'basic': basic_extractor,\n",
    "    'feats1': fe1,\n",
    "    'ohmed': oh_med,\n",
    "    'kmeans10': kmeans_10,\n",
    "    'kmeans20': kmeans_20,\n",
    "    'oh_km10': oh_km10,\n",
    "    'oh_km20': oh_km20,\n",
    "    'fe1_km10': fe1_km10,\n",
    "    'fe1_km20': fe1_km20\n",
    "}\n",
    "\n",
    "@memo(Perd(MEMO_PATH + \"feature_extraction\"))\n",
    "def train_test_sets(extractor_name):\n",
    "    \"\"\"extractor_name one of:\n",
    "    'basic'\n",
    "    'feats1'\n",
    "    returns X_train, X_test\n",
    "    \"\"\"\n",
    "    extractor = extractors[extractor_name]\n",
    "    return extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, X_test = train_test_sets(\"ohmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from feature_engineering import train_test_sets, oh_km10, oh_km20, kmeans_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ae643b4fa692>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ohmed'\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m#train_test_sets(\"oh_km20\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<decorator-gen-123>\u001b[0m in \u001b[0;36mtrain_test_sets\u001b[1;34m(extractor_name)\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/nadbor/kaggle/prudential/persistent_cache.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(f, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/nadbor/kaggle/prudential/persistent_cache.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/nadbor/kaggle/prudential/persistent_cache.pyc\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, protocol)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m     \u001b[0mPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPROTO\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mchr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave_tuple\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMARK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave_tuple\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMARK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave_string\u001b[1;34m(self, obj, pack)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBINSTRING\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTRING\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mStringType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_string\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x, xx = train_test_sets('ohmed')    #train_test_sets(\"oh_km20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_maybe = clusterer.fit_transform(tiny_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79146, 1726)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([X, X_test]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up-to-date with 'origin/master'.\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git reset HEAD <file>...\" to unstage)\n",
      "\n",
      "\tmodified:   train_utils.py\n",
      "\tmodified:   training.ipynb\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add/rm <file>...\" to update what will be committed)\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
      "\n",
      "\tmodified:   train.py\n",
      "\tdeleted:    train1.py\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\n",
      "\t.ipynb_checkpoints/\n",
      "\tcache/\n",
      "\tcachetest/\n",
      "\tfeature_engineering.py\n",
      "\tfeature_engineering.pyc\n",
      "\tmemo/\n",
      "\tmemo_fe/\n",
      "\tmemo_fe_lazy_stacker_train_predictions\n",
      "\tmemo_fe_stacker_test_predictions\n",
      "\tmemo_fe_stacker_train_predictions\n",
      "\tmemo_fe_test_predictions\n",
      "\tmemo_fe_train_predictions\n",
      "\tpersistent_cache.pyc\n",
      "\ttrain_utils-Copy1.py\n",
      "\ttrain_utils.pyc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-6ba9a3609c49>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-6ba9a3609c49>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    git commit -m \"asfs\"\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git commit -m \"asfs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
