{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data using pandas\n",
      "Eliminate missing values\n"
     ]
    }
   ],
   "source": [
    "MEMO_PATH = \"memo_fe/\"\n",
    "\n",
    "#=========================================LOGGING\n",
    "import logging\n",
    "# create logger\n",
    "logging.basicConfig(filename='feat_engineered.log',level=logging.DEBUG, format=\"%(asctime)s; %(levelname)s;  %(message)s\")\n",
    "logger = logging.getLogger(\"trainlo\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "def info(msg):\n",
    "    logger.info(msg.replace(\"\\n\", \"  \"))\n",
    "#=========================================LOGGING\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.optimize import fmin_powell\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier, \\\n",
    "    SGDRegressor, Perceptron, PassiveAggressiveRegressor, BayesianRidge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, SVR\n",
    "from xgboost.sklearn import XGBClassifier, XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from persistent_cache import memo, PersistentDict as Perd\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "\n",
    "def eval_wrapper(yhat, y):  \n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n",
    "    return quadratic_weighted_kappa(yhat, y)\n",
    "\n",
    "\n",
    "num_classes = 8\n",
    "\n",
    "\n",
    "print(\"Load the data using pandas\")\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# combine train and test\n",
    "all_data = train.append(test)\n",
    "\n",
    "# factorize categorical variables    \n",
    "all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\n",
    "\n",
    "# FEATURE ENGINEERING\n",
    "all_data['bmi_ins_age'] = all_data.BMI * all_data.Ins_Age\n",
    "all_data['nan_count'] = all_data.isnull().sum(axis=1)\n",
    "#all_data['emp_inf_4_sq'] = all_data.Employment_Info_4 ** 2\n",
    "#all_data['fam_hist_4_sq'] = all_data.Family_Hist_4 ** 2\n",
    "#all_data['fam_hist_2_sq'] = all_data.Family_Hist_2 ** 2\n",
    "\n",
    "mk = [col for col in train.columns if col.startswith(\"Medical_K\")]\n",
    "all_data['sum_keywords'] = sum(train[col] for col in mk)\n",
    "\n",
    "all_data.drop('Medical_History_24')\n",
    "all_data.drop('Medical_History_10')\n",
    "\n",
    "\n",
    "\n",
    "print('Eliminate missing values')    \n",
    "# Use -1 for any others\n",
    "all_data.fillna(-1, inplace=True)\n",
    "\n",
    "# fix the dtype on the label column\n",
    "all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "# Provide split column\n",
    "# all_data['Split'] = np.random.randint(5, size=all_data.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train = all_data[all_data['Response']>0].copy()\n",
    "test = all_data[all_data['Response']<1].copy()\n",
    "\n",
    "\n",
    "X = np.array(train.drop([\"Id\", \"Response\"], axis=1))\n",
    "X_actual_test = np.array(test.drop([\"Id\", \"Response\"], axis=1))\n",
    "y = np.array(train.Response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_test_folds = list(StratifiedKFold(y, n_folds=4, random_state=0))\n",
    "#================================================================================================\n",
    "@memo(Perd(MEMO_PATH + \"_train_predictions\"))\n",
    "def train_predictions(model):\n",
    "    ind2pred = {}\n",
    "    for i, (train, test) in enumerate(train_test_folds):\n",
    "        info((\"fitting fold   \"+str(i+1)+ str(model)[:100]))\n",
    "        model.fit(X[train], y[train])\n",
    "        info((\"fold fitted    \"+str(i+1)+  str(model)[:100]))\n",
    "        preds = model.predict(X[test])\n",
    "        for i, p in zip(test, preds):\n",
    "            ind2pred[i] = p\n",
    "    \n",
    "    return np.array([ind2pred[i] for i in range(len(y))])\n",
    "\n",
    "@memo(Perd(MEMO_PATH + \"_test_predictions\"))\n",
    "def test_predictions(model):\n",
    "    info(\"fitting (on full train set) %s\" % model)\n",
    "    model.fit(X, y)\n",
    "    info(\"done fitting for %s\" % model)\n",
    "    return model.predict(X_actual_test)\n",
    "\n",
    "\n",
    "@memo(Perd(MEMO_PATH + \"_stacker_train_predictions\"))\n",
    "def stacker_train_predictions(stacker, base_clfs):\n",
    "    info(\"start stacker --------------------------\")\n",
    "    n = len(y)\n",
    "    stacked_X = np.hstack([X] + [train_predictions(clf).reshape(n, 1) for clf in base_clfs])\n",
    "    info(\"base regressors done\")\n",
    "    ind2pred = {}\n",
    "    for i, (train, test) in enumerate(train_test_folds):\n",
    "        info(\"fitting stacker fold %s   %s\" % (i, str(stacker)))\n",
    "\n",
    "        stacker.fit(stacked_X[train], y[train])\n",
    "        info(\"stacker fitted fold %s    %s \" % (i, str(stacker)))\n",
    "        preds = stacker.predict(stacked_X[test])\n",
    "        for i, p in zip(test, preds):\n",
    "            ind2pred[i] = p\n",
    "    info(\"stacker done =========================\")\n",
    "    return np.array([ind2pred[i] for i in range(len(y))])\n",
    "\n",
    "@memo(Perd(MEMO_PATH + \"_lazy_stacker_train_predictions\"))\n",
    "def lazy_stacker_train_predictions(stacker, base_clfs):\n",
    "    info(\"start stacker --------------------------\")\n",
    "    n = len(y)\n",
    "    stacked_X = np.hstack([train_predictions(clf).reshape(n, 1) for clf in base_clfs])\n",
    "    info(\"base regressors done\")\n",
    "    ind2pred = {}\n",
    "    for i, (train, test) in enumerate(train_test_folds):\n",
    "        info(\"fitting stacker fold %s   %s\" % (i, str(stacker)))\n",
    "\n",
    "        stacker.fit(stacked_X[train], y[train])\n",
    "        info(\"stacker fitted fold %s    %s \" % (i, str(stacker)))\n",
    "        preds = stacker.predict(stacked_X[test])\n",
    "        for i, p in zip(test, preds):\n",
    "            ind2pred[i] = p\n",
    "    info(\"stacker done =========================\")\n",
    "    return np.array([ind2pred[i] for i in range(len(y))])\n",
    "\n",
    "\n",
    "@memo(Perd(MEMO_PATH + \"_stacker_test_predictions\"))\n",
    "def stacker_test_predictions(stacker, base_clfs):\n",
    "    n = len(y)\n",
    "    print \"train length = %s\" % n\n",
    "    stacked_X = np.hstack([X] + [train_predictions(clf).reshape(n, 1) for clf in base_clfs])\n",
    "    stacker.fit(stacked_X, y)\n",
    "    nn = X_actual_test.shape[0]\n",
    "    print \"test length = %s\" % nn\n",
    "    stacked_test_X = np.hstack([X_actual_test] + [test_predictions(clf).reshape(nn, 1) for clf in base_clfs])\n",
    "    return stacker.predict(stacked_test_X)\n",
    "#============================================================================\n",
    "def benchmark(model):\n",
    "    pred = train_predictions(model)\n",
    "    return eval_wrapper(pred, y)\n",
    "\n",
    "def make_predictions(model):\n",
    "    model.fit(X, y)\n",
    "    return model.predict(X_actual_test)\n",
    "\n",
    "def benchmark_stacker(model, base_clfs):\n",
    "    pred = stacker_train_predictions(model, base_clfs)\n",
    "    result = eval_wrapper(pred, y)\n",
    "    info(\"stacker %s   %s, %s\" % (result, model, base_clfs))\n",
    "    return result\n",
    "\n",
    "def benchmark_lazy_stacker(model, base_clfs):\n",
    "    pred = lazy_stacker_train_predictions(model, base_clfs)\n",
    "    result = eval_wrapper(pred, y)\n",
    "    info(\"lazy stacker %s   %s, %s\" % (result, model, base_clfs))\n",
    "    \n",
    "    return result\n",
    "#==============================================================================\n",
    "# OPTIMISING OFFSETS\n",
    "# -----------------------------------------------------------------------------\n",
    "def apply_offset(data, bin_offset, sv, scorer=eval_wrapper):\n",
    "    # data has the format of pred=0, offset_pred=1, labels=2 in the first dim\n",
    "    data[1, data[0].astype(int)==sv] = data[0, data[0].astype(int)==sv] + bin_offset\n",
    "    score = scorer(data[1], data[2])\n",
    "    return score\n",
    "\n",
    "\n",
    "def optimize_offsets(predictions, y):\n",
    "    # train offsets\n",
    "    info(\"optimising offsets %s\" % len(y))\n",
    "    offsets = np.ones(num_classes) * -0.5\n",
    "    offset_train_preds = np.vstack((predictions, predictions, y))\n",
    "    \n",
    "    for j in range(num_classes):\n",
    "        train_offset = lambda x: -apply_offset(offset_train_preds, x, j)\n",
    "        offsets[j] = fmin_powell(train_offset, offsets[j], disp=False) \n",
    "    info(\"done optimising offsets %s\" % len(y))\n",
    "    return offsets\n",
    "\n",
    "def actually_apply_offsets(predictions, offsets):\n",
    "    data = np.vstack((predictions, predictions, -np.ones(len(predictions))))\n",
    "    for j in range(num_classes):\n",
    "        data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j] \n",
    "\n",
    "    final_test_preds = np.round(np.clip(data[1], 1, 8)).astype(int)\n",
    "    return final_test_preds\n",
    "\n",
    "def optimized_train_predictions(raw_train_predictions):\n",
    "    n = len(y)\n",
    "    ind2pred = {}\n",
    "    for i, (train, test) in enumerate(train_test_folds):\n",
    "        train_preds = raw_train_predictions[train]\n",
    "        offsets = optimize_offsets(train_preds, y[train])\n",
    "        test_preds = actually_apply_offsets(raw_train_predictions[test], offsets)\n",
    "        for i, p in zip(test, test_preds):\n",
    "            ind2pred[i] = p\n",
    "    return np.array([ind2pred[i] for i in range(len(y))])\n",
    "\n",
    "def benchmark_model_optimized(model):\n",
    "    preds = optimized_train_predictions(train_predictions(model))\n",
    "    result = eval_wrapper(preds, y)\n",
    "    info(\"optimized %s   %s\" % (result, model))\n",
    "    return result\n",
    "\n",
    "def benchmark_optimized_stacker(stacker, base_clfs):\n",
    "    preds = stacker_train_predictions(stacker, base_clfs)\n",
    "    opreds = optimized_train_predictions(preds)\n",
    "    result = eval_wrapper(opreds, y)\n",
    "    info(\"optimized stacker %s   %s, %s\" % (result, stacker, base_clfs))\n",
    "    return result\n",
    "\n",
    "def benchmark_optimized_lazy_stacker(stacker, base_clfs):\n",
    "    preds = lazy_stacker_train_predictions(stacker, base_clfs)\n",
    "    opreds = optimized_train_predictions(preds)\n",
    "    result = eval_wrapper(opreds, y)\n",
    "    info(\"optimized stacker %s   %s, %s\" % (result, stacker, base_clfs))\n",
    "    return result\n",
    "\n",
    "def optimized_test_predictions(stacker, base_clfs):\n",
    "    train_preds = stacker_train_predictions(stacker, base_clfs)\n",
    "    offsets = optimize_offsets(train_preds, y)\n",
    "    test_preds = stacker_test_predictions(stacker, base_clfs)\n",
    "    final_test_preds = actually_apply_offsets(test_preds, offsets)\n",
    "    #print \"print len(train_preds), len(test_preds), len(final_test_preds)\"\n",
    "    #print len(train_preds), len(test_preds), len(final_test_preds)\n",
    "    return final_test_preds\n",
    "\n",
    "def make_sub_optimized(stacker, base_clfs, filename):\n",
    "    preds = optimized_test_predictions(stacker, base_clfs)\n",
    "    #print \"len(preds)\"\n",
    "    #print len(preds)\n",
    "    df = pd.DataFrame()\n",
    "    df['Id'] = test.Id\n",
    "    df['Response'] = preds\n",
    "\n",
    "    df.to_csv(filename, index=False)\n",
    "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "xgbr = lambda: XGBRegressor(objective=\"reg:linear\", min_child_weight=80, subsample=0.85, colsample_bytree=0.30, silent=1, max_depth=9)\n",
    "xgbc = lambda: XGBClassifier(objective=\"reg:linear\", min_child_weight=80, subsample=0.85, colsample_bytree=0.30, silent=1, max_depth=9)\n",
    "rfr = lambda: RandomForestRegressor(n_estimators=400)\n",
    "etr = lambda: ExtraTreesRegressor(n_estimators=400)\n",
    "etc = lambda: ExtraTreesClassifier(n_estimators=400)\n",
    "sgdr = lambda: SGDRegressor()\n",
    "xgbr_poly = lambda: Pipeline([(\"poly\", PolynomialFeatures(degree=2)), (\"xgbr\", xgbr())])\n",
    "linreg_poly = lambda: Pipeline([(\"poly\", PolynomialFeatures(degree=2)), (\"linreg\", LinearRegression())])\n",
    "linreg = lambda: LinearRegression()\n",
    "bayes_ridge = lambda: BayesianRidge()\n",
    "perceptron = lambda: Perceptron()\n",
    "lasso = lambda: Lasso()\n",
    "svrsig = lambda: SVR(kernel=\"sigmoid\")\n",
    "svrrbf = lambda: SVR(kernel=\"rbf\")\n",
    "perc = lambda: Perceptron()\n",
    "\n",
    "dream_team = lambda: sorted([xgbr(), rfr(),  etr(), LinearRegression(), Perceptron(), xgbr_poly(), linreg_poly(),\n",
    "                             bayes_ridge(),\n",
    "                             lasso(),\n",
    "                             svrsig(),\n",
    "                             svrrbf(),\n",
    "                             perc()\n",
    "                            ])\n",
    "\n",
    "def make_sub(stacker, base_clfs, filename):\n",
    "    preds = stacker_test_predictions(stacker, base_clfs)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['Id'] = test.Id\n",
    "    df['Response'] = preds\n",
    "\n",
    "    df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = sorted(dream_team())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[0, 2, 3, 4]\n",
      "[0, 1, 3, 4]\n",
      "[0, 1, 2, 4]\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "li = range(5)\n",
    "for i in range(len(li)):\n",
    "    print li[0:i] + li[i+1:len(li)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
